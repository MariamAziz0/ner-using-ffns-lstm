{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariamAziz0/ner-using-ffns-lstm/blob/main/A3P2_Named_Entity_Recognition_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f317c406-3c60-472c-cebd-32635405a96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 10 12:29:19 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1791fabf-438d-40ee-ec7a-2c9bb6472f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==2.0.1+cu118 in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.1.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1+cu118 torchtext==0.15.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe1f43f-bcfb-468f-8f21-0fe5e0ad53c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7918351-80e8-4199-a3a6-f7bcda4b0787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Using cached scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "Successfully installed scikit-learn-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "outputId": "d77fdb4c-864f-46b7-bbb1-a8d445471f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "\n",
        "        for i in range(batch_size):\n",
        "          for j in range(len(words[i])):\n",
        "            # handling word_idxs\n",
        "            valid_mask[i, j] = True     # always true because we are within the sentence length.\n",
        "            if words[i][j] in self.words_vocab:\n",
        "              word_idxs[i, j] = self.words_vocab[words[i][j]]\n",
        "            else:\n",
        "              word_idxs[i, j] = self.unknown_idx\n",
        "\n",
        "            # handling tag_idxs\n",
        "            if tags[i][j] in self.tags_vocab:\n",
        "              tag_idxs[i, j] = self.tags_vocab[tags[i][j]]\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TazmodGWYx2d",
        "outputId": "f4e1e153-62f3-4e15-f294-64993073cd66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "        self.embeddings = nn.Embedding(len(words_vocab), d_emb)\n",
        "        self.lstm = nn.LSTM(\n",
        "            d_emb,\n",
        "            d_hidden,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "            )\n",
        "        self.linear = nn.Linear((2 if bidirectional else 1) * d_hidden, len(tags_vocab))\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        embedded_words = self.embeddings(word_idxs)\n",
        "        lstm_op, _ = self.lstm(embedded_words)\n",
        "        logits = self.linear(lstm_op * valid_mask.unsqueeze(-1))\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "outputId": "367212f9-0894-4211-d396-66b718750c71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 26])\n",
            "Input valid_mask shape: torch.Size([4, 26])\n",
            "Output logits shape: torch.Size([4, 26, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        logits = model.forward(word_idxs, valid_mask)\n",
        "        loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask])\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "            logits = model.forward(word_idxs, valid_mask)\n",
        "            loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask])\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "outputId": "dcb63d2a-88d3-4448-9fa6-053aeee9e447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.7967879904641045\n",
            "Training metrics:\n",
            "\t accuracy :  0.7963467584967809\n",
            "\t f1 :  [0.18004228 0.88940274 0.01018503 0.17804278 0.21482904]\n",
            "\t average f1 :  0.2945003754179274\n",
            "\t confusion matrix :  [[  1235   7804     43    431    327]\n",
            " [  2248 155199   1213   5622   2626]\n",
            " [    84   3986     30    155    255]\n",
            " [   203   8734     64   1702    233]\n",
            " [   109   6365     31    273   1398]]\n",
            "Validating..\n",
            "Validation loss:  0.35546925238200594\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8970785967485637\n",
            "\t f1 :  [0.45752466 0.95042481 0.03128055 0.58876298 0.6028024 ]\n",
            "\t average f1 :  0.5261590811276261\n",
            "\t confusion matrix :  [[  719  1230     0   122   179]\n",
            " [   31 40998     0    89    46]\n",
            " [   33   738    16    30   190]\n",
            " [   55  1335     0  1247    53]\n",
            " [   55   808     0    58  1054]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.25435710339634504\n",
            "Training metrics:\n",
            "\t accuracy :  0.9224566282413568\n",
            "\t f1 :  [0.61555037 0.96909359 0.41775033 0.69848439 0.71593911]\n",
            "\t average f1 :  0.683363557261982\n",
            "\t confusion matrix :  [[  5142   3004    147    883    699]\n",
            " [   357 166013     57    464    245]\n",
            " [   394   1657   1285    304    857]\n",
            " [   513   3228     35   6959    250]\n",
            " [   426   1577    131    331   5691]]\n",
            "Validating..\n",
            "Validation loss:  0.20799534022808075\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9371918673348817\n",
            "\t f1 :  [0.66363636 0.9765072  0.56403941 0.77038145 0.76559781]\n",
            "\t average f1 :  0.7480324480295248\n",
            "\t confusion matrix :  [[ 1314   513    50   173   200]\n",
            " [   93 40631    41   274   125]\n",
            " [   91   268   458    79   111]\n",
            " [   96   432    30  2060    72]\n",
            " [  116   209    38    72  1540]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.1296123699457557\n",
            "Training metrics:\n",
            "\t accuracy :  0.9617958018790259\n",
            "\t f1 :  [0.79739112 0.98854323 0.72629258 0.86448729 0.84643635]\n",
            "\t average f1 :  0.8446301122932018\n",
            "\t confusion matrix :  [[  7580   1032    220    555    471]\n",
            " [   323 166141    127    274    106]\n",
            " [   360    557   2964    219    425]\n",
            " [   494    921    108   9282    137]\n",
            " [   397    511    218    202   6799]]\n",
            "Validating..\n",
            "Validation loss:  0.16792056815964834\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9472558366947806\n",
            "\t f1 :  [0.7455814  0.98075192 0.67670011 0.79067493 0.83004795]\n",
            "\t average f1 :  0.8047512597232082\n",
            "\t confusion matrix :  [[ 1603   288    42   236    81]\n",
            " [  140 40304    75   570    75]\n",
            " [   85   166   607   106    43]\n",
            " [   75   150    18  2425    22]\n",
            " [  147   118    45   107  1558]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.07456428143713209\n",
            "Training metrics:\n",
            "\t accuracy :  0.9793266685975092\n",
            "\t f1 :  [0.88374486 0.99445446 0.83664408 0.93964768 0.91039604]\n",
            "\t average f1 :  0.9129774216937777\n",
            "\t confusion matrix :  [[  8590    505    170    301    280]\n",
            " [   229 166324     71    116     52]\n",
            " [   231    337   3580    131    249]\n",
            " [   249    302     50  10268     67]\n",
            " [   295    243    159    103   7356]]\n",
            "Validating..\n",
            "Validation loss:  0.15624816289969853\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9511673389561178\n",
            "\t f1 :  [0.76501634 0.98221568 0.6873805  0.83225468 0.81455233]\n",
            "\t average f1 :  0.8162839029776547\n",
            "\t confusion matrix :  [[ 1522   275    84   158   211]\n",
            " [   68 40345   178   376   197]\n",
            " [   22   144   719    63    59]\n",
            " [   50   143    51  2379    67]\n",
            " [   67    80    53    51  1724]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.04621532242055292\n",
            "Training metrics:\n",
            "\t accuracy :  0.987560959782766\n",
            "\t f1 :  [0.9287932  0.9968057  0.89444126 0.97131409 0.94070031]\n",
            "\t average f1 :  0.9464109132538907\n",
            "\t confusion matrix :  [[  9124    313    108    154    205]\n",
            " [   147 166483     58     51     33]\n",
            " [   157    218   3902     69    165]\n",
            " [    91    133     26  10666     51]\n",
            " [   224    114    120     55   7670]]\n",
            "Validating..\n",
            "Validation loss:  0.1504472547343799\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9541620828749542\n",
            "\t f1 :  [0.7890572  0.98247497 0.74654887 0.81574595 0.85699615]\n",
            "\t average f1 :  0.8381646268525644\n",
            "\t confusion matrix :  [[ 1745   220    26   171    88]\n",
            " [  160 40280    74   557    93]\n",
            " [   63   148   676    80    40]\n",
            " [   85   100     9  2466    30]\n",
            " [  120    85    19    82  1669]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.03004063424412851\n",
            "Training metrics:\n",
            "\t accuracy :  0.9923079992816809\n",
            "\t f1 :  [0.95442715 0.99789377 0.93378608 0.98674682 0.96252224]\n",
            "\t average f1 :  0.9670752106511724\n",
            "\t confusion matrix :  [[  9351    204     68     66    132]\n",
            " [   122 166771     41     22     21]\n",
            " [    99    160   4125     31    103]\n",
            " [    53     54     11  10833     23]\n",
            " [   149     80     72     31   7846]]\n",
            "Validating..\n",
            "Validation loss:  0.1610471765909876\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9542232000977876\n",
            "\t f1 :  [0.79589342 0.98241421 0.76247289 0.80177187 0.86574783]\n",
            "\t average f1 :  0.8416600433825424\n",
            "\t confusion matrix :  [[ 1628   240    33   237   112]\n",
            " [   75 40278    70   663    78]\n",
            " [   26   142   703   101    35]\n",
            " [   41    84     9  2534    22]\n",
            " [   71    90    22    96  1696]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.020349889131331886\n",
            "Training metrics:\n",
            "\t accuracy :  0.9950847457627119\n",
            "\t f1 :  [0.97073294 0.99867779 0.95792662 0.9921224  0.97518838]\n",
            "\t average f1 :  0.9789296277190825\n",
            "\t confusion matrix :  [[  9569    134     40     37     95]\n",
            " [    72 166923     27     12     14]\n",
            " [    69    106   4269     15     56]\n",
            " [    37     22     10  10894     18]\n",
            " [    93     55     52     22   7959]]\n",
            "Validating..\n",
            "Validation loss:  0.1600182375737599\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9560159719675672\n",
            "\t f1 :  [0.80598389 0.98279434 0.75284385 0.82014388 0.86930746]\n",
            "\t average f1 :  0.8462146843861756\n",
            "\t confusion matrix :  [[ 1751   224    45   172    58]\n",
            " [  124 40327   105   570    38]\n",
            " [   39   138   728    86    16]\n",
            " [   54   106    11  2508    11]\n",
            " [  127   107    38    90  1613]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.013474676585583776\n",
            "Training metrics:\n",
            "\t accuracy :  0.997059659020119\n",
            "\t f1 :  [0.98395505 0.99906099 0.97437611 0.99611713 0.98563889]\n",
            "\t average f1 :  0.9878296345556515\n",
            "\t confusion matrix :  [[  9720     83     25     13     43]\n",
            " [    52 167041     27      5      7]\n",
            " [    32     77   4373      9     33]\n",
            " [    15     15      3  10903     12]\n",
            " [    54     48     24     13   8030]]\n",
            "Validating..\n",
            "Validation loss:  0.16169950472457068\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9571975716090128\n",
            "\t f1 :  [0.81009043 0.98308717 0.75539568 0.82531312 0.87490206]\n",
            "\t average f1 :  0.8497576935298596\n",
            "\t confusion matrix :  [[ 1702   239    50   180    79]\n",
            " [   92 40369   104   538    61]\n",
            " [   25   149   735    78    20]\n",
            " [   46   108    13  2504    19]\n",
            " [   87    98    37    78  1675]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.010216984984085516\n",
            "Training metrics:\n",
            "\t accuracy :  0.997817181472954\n",
            "\t f1 :  [0.98726501 0.99927317 0.98130009 0.99762687 0.99012452]\n",
            "\t average f1 :  0.9911179314525282\n",
            "\t confusion matrix :  [[  9768     72     21      9     29]\n",
            " [    49 167043     15      3      9]\n",
            " [    26     54   4408      6     24]\n",
            " [    10      9      3  10930      7]\n",
            " [    36     32     19      5   8071]]\n",
            "Validating..\n",
            "Validation loss:  0.1632366861615862\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9596015157071263\n",
            "\t f1 :  [0.81766917 0.98353015 0.76419437 0.84259259 0.87847966]\n",
            "\t average f1 :  0.8572931901149431\n",
            "\t confusion matrix :  [[ 1740   278    45   125    62]\n",
            " [   90 40518   105   416    35]\n",
            " [   22   155   747    70    13]\n",
            " [   57   155    11  2457    10]\n",
            " [   97   123    40    74  1641]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.008426860319795433\n",
            "Training metrics:\n",
            "\t accuracy :  0.9980101636253921\n",
            "\t f1 :  [0.98806272 0.99948224 0.98443753 0.99807816 0.98729048]\n",
            "\t average f1 :  0.9914702240367032\n",
            "\t confusion matrix :  [[  9767     51     15      5     46]\n",
            " [    30 166979     12      4      7]\n",
            " [    23     34   4428      5     28]\n",
            " [     5      6      2  10906      9]\n",
            " [    61     29     21      6   8040]]\n",
            "Validating..\n",
            "Validation loss:  0.1658728261079107\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9591940675549037\n",
            "\t f1 :  [0.81134402 0.98336463 0.71232877 0.85673455 0.88508171]\n",
            "\t average f1 :  0.8497707352101503\n",
            "\t confusion matrix :  [[ 1645   314   107   110    74]\n",
            " [   58 40581   187   305    33]\n",
            " [    8   157   780    46    16]\n",
            " [   37   199    39  2398    17]\n",
            " [   57   120    70    49  1679]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.005218908276960805\n",
            "Training metrics:\n",
            "\t accuracy :  0.9991030630397194\n",
            "\t f1 :  [0.9953446  0.99968598 0.99268941 0.99885725 0.99558932]\n",
            "\t average f1 :  0.9964333112921882\n",
            "\t confusion matrix :  [[  9835     25     12      7     10]\n",
            " [    19 167135      8      1      7]\n",
            " [     3     22   4481      3     10]\n",
            " [     2      5      1  10926      4]\n",
            " [    14     18      7      2   8126]]\n",
            "Validating..\n",
            "Validation loss:  0.17313989251852036\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9618221081367396\n",
            "\t f1 :  [0.81738708 0.98370163 0.78590216 0.85668219 0.88427144]\n",
            "\t average f1 :  0.8655888988273098\n",
            "\t confusion matrix :  [[ 1683   337    43   113    74]\n",
            " [   65 40710    75   274    40]\n",
            " [   14   174   747    55    17]\n",
            " [   36   240     8  2391    15]\n",
            " [   70   144    21    59  1681]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.003438696674830108\n",
            "Training metrics:\n",
            "\t accuracy :  0.9995218837779151\n",
            "\t f1 :  [0.99782003 0.99980272 0.99634996 0.99931522 0.99785893]\n",
            "\t average f1 :  0.9982293741241385\n",
            "\t confusion matrix :  [[  9841     14      1      8      2]\n",
            " [     8 167246      5      0      4]\n",
            " [     2     17   4504      1      3]\n",
            " [     2      3      0  10945      1]\n",
            " [     6     15      4      0   8156]]\n",
            "Validating..\n",
            "Validation loss:  0.17743384199483053\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9625555148107403\n",
            "\t f1 :  [0.82085308 0.98350297 0.78459034 0.86504548 0.88730911]\n",
            "\t average f1 :  0.8682601957782664\n",
            "\t confusion matrix :  [[ 1732   343    29    79    67]\n",
            " [   84 40778    62   205    35]\n",
            " [   24   199   723    41    20]\n",
            " [   48   292     4  2330    16]\n",
            " [   82   148    18    42  1685]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.0025012034198476207\n",
            "Training metrics:\n",
            "\t accuracy :  0.9997405798041378\n",
            "\t f1 :  [0.99893406 0.99988917 0.99811676 0.99972697 0.99859181]\n",
            "\t average f1 :  0.9990517564149141\n",
            "\t confusion matrix :  [[  9840      8      0      3      1]\n",
            " [     3 166910      1      0      4]\n",
            " [     1     11   4505      0      2]\n",
            " [     1      1      0  10985      1]\n",
            " [     4      9      2      0   8155]]\n",
            "Validating..\n",
            "Validation loss:  0.1797852431024824\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9619239701747953\n",
            "\t f1 :  [0.82341697 0.98389341 0.78288431 0.85339943 0.88737201]\n",
            "\t average f1 :  0.8661932272824417\n",
            "\t confusion matrix :  [[ 1723   307    37   117    66]\n",
            " [   81 40653    81   310    39]\n",
            " [   17   169   741    60    20]\n",
            " [   43   212     6  2410    19]\n",
            " [   71   132    21    61  1690]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.03768662913029806\n",
            "Training metrics:\n",
            "\t accuracy :  0.9899694744822632\n",
            "\t f1 :  [0.95365903 0.9948937  0.97074086 0.96063839 0.98445406]\n",
            "\t average f1 :  0.9728772101467993\n",
            "\t confusion matrix :  [[  9487    228     36     15     88]\n",
            " [   469 165806     97    573     43]\n",
            " [    31     57   4396      9     14]\n",
            " [    14    209     18  10714     24]\n",
            " [    41     26      3     16   8074]]\n",
            "Validating..\n",
            "Validation loss:  0.14457820568765914\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9631666870390743\n",
            "\t f1 :  [0.82152169 0.98400549 0.80151025 0.86212239 0.88189386]\n",
            "\t average f1 :  0.8702107339968539\n",
            "\t confusion matrix :  [[ 1733   347    24    57    89]\n",
            " [   82 40881    55   109    37]\n",
            " [   24   187   743    38    15]\n",
            " [   54   369     8  2226    33]\n",
            " [   76   143    17    44  1695]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.009032864451270413\n",
            "Training metrics:\n",
            "\t accuracy :  0.9977585079350816\n",
            "\t f1 :  [0.98673443 0.99901441 0.98568099 0.99646708 0.99375612]\n",
            "\t average f1 :  0.9923306067289385\n",
            "\t confusion matrix :  [[  9707    101     20     13     20]\n",
            " [    72 166741     21     11     15]\n",
            " [    12     59   4440      4      9]\n",
            " [     7     26      1  10859      7]\n",
            " [    16     24      3      8   8117]]\n",
            "Validating..\n",
            "Validation loss:  0.15771818586758204\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9650409485392983\n",
            "\t f1 :  [0.82646213 0.98433515 0.80771291 0.8767864  0.89346184]\n",
            "\t average f1 :  0.8777516851966528\n",
            "\t confusion matrix :  [[ 1724   359    30    54    83]\n",
            " [   72 40907    60    96    29]\n",
            " [   18   192   754    29    14]\n",
            " [   47   348     2  2270    23]\n",
            " [   61   146    14    39  1715]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "outputId": "8e949453-2852-42f3-ab46-fde3bf518c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (linear): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.5670523566228373\n",
            "Training metrics:\n",
            "\t accuracy :  0.8115941306396366\n",
            "\t f1 :  [0.09349962 0.9036335  0.0827779  0.32493494 0.36325459]\n",
            "\t average f1 :  0.3536201086273956\n",
            "\t confusion matrix :  [[   671   7344    339    872    626]\n",
            " [  3240 156367   3519   2422   1574]\n",
            " [   162   3293    385    448    254]\n",
            " [   205   7179    274   2934    312]\n",
            " [   223   4780    243    479   2422]]\n",
            "Validating..\n",
            "Validation loss:  0.3202314760003771\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9009289817870676\n",
            "\t f1 :  [0.37484737 0.95872011 0.38534799 0.57112391 0.62918796]\n",
            "\t average f1 :  0.5838454682229878\n",
            "\t confusion matrix :  [[  614  1102    33   260   241]\n",
            " [   77 40899    23   107    58]\n",
            " [  105   424   263   143    72]\n",
            " [   56  1218     9  1339    68]\n",
            " [  174   513    30   150  1108]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.21414455661067255\n",
            "Training metrics:\n",
            "\t accuracy :  0.9315507540849388\n",
            "\t f1 :  [0.59962385 0.97834052 0.56479186 0.75275444 0.73201871]\n",
            "\t average f1 :  0.7255058736104015\n",
            "\t confusion matrix :  [[  5420   2144    406    967    938]\n",
            " [   595 165161    168    595    273]\n",
            " [   657    895   2164    424    390]\n",
            " [   687   1767    166   8062    272]\n",
            " [   844    876    229    418   5791]]\n",
            "Validating..\n",
            "Validation loss:  0.2350977063179016\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9308560485678198\n",
            "\t f1 :  [0.59742985 0.97162658 0.62192394 0.72600127 0.75076751]\n",
            "\t average f1 :  0.733549828826995\n",
            "\t confusion matrix :  [[ 1139   710    94   138   169]\n",
            " [  102 40939    44    55    24]\n",
            " [   67   286   556    59    39]\n",
            " [   59   863    24  1713    31]\n",
            " [  196   307    63    64  1345]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.12036586359695152\n",
            "Training metrics:\n",
            "\t accuracy :  0.9630684368547575\n",
            "\t f1 :  [0.77036055 0.99062889 0.74136518 0.89320925 0.83482506]\n",
            "\t average f1 :  0.8460777856441094\n",
            "\t confusion matrix :  [[  7553    845    408    433    659]\n",
            " [   455 166019    122    260    109]\n",
            " [   512    424   3166    186    234]\n",
            " [   410    598    113   9694    129]\n",
            " [   781    328    210    189   6669]]\n",
            "Validating..\n",
            "Validation loss:  0.21527708853994096\n",
            "Validation metrics:\n",
            "\t accuracy :  0.939106873650328\n",
            "\t f1 :  [0.66396556 0.97497917 0.68336116 0.75383618 0.7780417 ]\n",
            "\t average f1 :  0.7708367532278532\n",
            "\t confusion matrix :  [[ 1311   593    66    90   190]\n",
            " [   89 40954    46    41    34]\n",
            " [   74   252   614    28    39]\n",
            " [   57   790    22  1744    77]\n",
            " [  168   257    42    34  1474]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.07713860507916522\n",
            "Training metrics:\n",
            "\t accuracy :  0.9768314936133298\n",
            "\t f1 :  [0.84966857 0.99484247 0.83011144 0.9419089  0.88601616]\n",
            "\t average f1 :  0.900509508009171\n",
            "\t confusion matrix :  [[  8396    461    299    231    490]\n",
            " [   298 166465     83    138     64]\n",
            " [   346    254   3650    114    181]\n",
            " [   243    270     76  10288     83]\n",
            " [   603    158    141    114   7128]]\n",
            "Validating..\n",
            "Validation loss:  0.21065519324370793\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9450759890803895\n",
            "\t f1 :  [0.68363064 0.97699271 0.71571906 0.79461698 0.80606216]\n",
            "\t average f1 :  0.7954043112562201\n",
            "\t confusion matrix :  [[ 1303   561    57   108   221]\n",
            " [   63 40957    46    53    45]\n",
            " [   45   245   642    24    51]\n",
            " [   40   686    13  1919    32]\n",
            " [  111   230    29    36  1569]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.054554060101509094\n",
            "Training metrics:\n",
            "\t accuracy :  0.9837510726187862\n",
            "\t f1 :  [0.88661258 0.99679944 0.87370929 0.96896457 0.91249305]\n",
            "\t average f1 :  0.9277157854292207\n",
            "\t confusion matrix :  [[  8742    287    246    136    432]\n",
            " [   230 166623     57     73     33]\n",
            " [   281    164   3850     65    157]\n",
            " [   132    135     34  10584     44]\n",
            " [   492     91    109     59   7388]]\n",
            "Validating..\n",
            "Validation loss:  0.20306558055537088\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9482337122601149\n",
            "\t f1 :  [0.71022291 0.97802198 0.72786529 0.81136738 0.81983385]\n",
            "\t average f1 :  0.8094622814373033\n",
            "\t confusion matrix :  [[ 1386   537    54    87   186]\n",
            " [   71 40940    54    51    48]\n",
            " [   41   233   670    23    40]\n",
            " [   47   634    15  1970    24]\n",
            " [  108   212    41    35  1579]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.038990367479898314\n",
            "Training metrics:\n",
            "\t accuracy :  0.9885922088273389\n",
            "\t f1 :  [0.91923894 0.99780946 0.90841834 0.98125428 0.93705003]\n",
            "\t average f1 :  0.9487542095714916\n",
            "\t confusion matrix :  [[  9083    218    193     79    301]\n",
            " [   168 166944     35     45     29]\n",
            " [   209    108   4052     42    118]\n",
            " [    77     72     21  10757     30]\n",
            " [   351     58     91     45   7614]]\n",
            "Validating..\n",
            "Validation loss:  0.19953174144029617\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9504339322821171\n",
            "\t f1 :  [0.72873116 0.9792725  0.72650027 0.83163057 0.82158763]\n",
            "\t average f1 :  0.8175444260898177\n",
            "\t confusion matrix :  [[ 1499   475    62    69   145]\n",
            " [  106 40867    63    54    74]\n",
            " [   49   226   684    16    32]\n",
            " [   71   544    17  2035    23]\n",
            " [  139   188    50    30  1568]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.02962211450492894\n",
            "Training metrics:\n",
            "\t accuracy :  0.9913025295848414\n",
            "\t f1 :  [0.9377112  0.99835085 0.93096911 0.98813652 0.94919622]\n",
            "\t average f1 :  0.9608727806152704\n",
            "\t confusion matrix :  [[  9296    176    138     48    262]\n",
            " [   134 167083     38     23     24]\n",
            " [   153     80   4174     28     96]\n",
            " [    51     38     12  10828     27]\n",
            " [   273     39     74     33   7735]]\n",
            "Validating..\n",
            "Validation loss:  0.20394943867410933\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9494560567167828\n",
            "\t f1 :  [0.72094709 0.9789978  0.73279133 0.83271681 0.81433869]\n",
            "\t average f1 :  0.8159583438717899\n",
            "\t confusion matrix :  [[ 1492   449    51    63   195]\n",
            " [  153 40764    60    56   131]\n",
            " [   55   214   676    11    51]\n",
            " [   71   526    21  2026    46]\n",
            " [  118   160    30    20  1647]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.02319897635391465\n",
            "Training metrics:\n",
            "\t accuracy :  0.9930243797926248\n",
            "\t f1 :  [0.94763471 0.99870503 0.94428349 0.99284901 0.95867263]\n",
            "\t average f1 :  0.9684289719977967\n",
            "\t confusion matrix :  [[  9365    146    125     28    218]\n",
            " [   122 166969     27     11     22]\n",
            " [   124     58   4237     14     86]\n",
            " [    41     23      6  10899     16]\n",
            " [   231     24     60     18   7829]]\n",
            "Validating..\n",
            "Validation loss:  0.21091281941958837\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9498635048690054\n",
            "\t f1 :  [0.72838599 0.97904314 0.72678762 0.83462955 0.8139477 ]\n",
            "\t average f1 :  0.8165587997463817\n",
            "\t confusion matrix :  [[ 1487   453    55    69   186]\n",
            " [  122 40784    76    50   132]\n",
            " [   48   217   681    15    46]\n",
            " [   61   529    19  2039    42]\n",
            " [  115   167    36    23  1634]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.0189027762454417\n",
            "Training metrics:\n",
            "\t accuracy :  0.9941949556510584\n",
            "\t f1 :  [0.95625729 0.9988345  0.95551601 0.99456249 0.96599056]\n",
            "\t average f1 :  0.9742321689347623\n",
            "\t confusion matrix :  [[  9433    138     97     23    181]\n",
            " [   111 166686     28      9     21]\n",
            " [    97     45   4296      8     70]\n",
            " [    29     16      7  10883     13]\n",
            " [   187     21     48     14   7882]]\n",
            "Validating..\n",
            "Validation loss:  0.21570293392453874\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9411644868190523\n",
            "\t f1 :  [0.68463279 0.9767504  0.68782518 0.82030679 0.79460352]\n",
            "\t average f1 :  0.7928237390640993\n",
            "\t confusion matrix :  [[ 1664   335    46   104   101]\n",
            " [  426 40184   138   335    81]\n",
            " [  130   147   661    48    21]\n",
            " [   94   325    14  2246    11]\n",
            " [  297   126    56    53  1443]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.0169910435323362\n",
            "Training metrics:\n",
            "\t accuracy :  0.9948243937253884\n",
            "\t f1 :  [0.96181276 0.99890023 0.96387154 0.99456348 0.96894637]\n",
            "\t average f1 :  0.9776188769087121\n",
            "\t confusion matrix :  [[  9508    120     77     15    186]\n",
            " [    98 166670     22     15     27]\n",
            " [    81     32   4322      4     55]\n",
            " [    23     35      6  10885     12]\n",
            " [   155     18     47      9   7941]]\n",
            "Validating..\n",
            "Validation loss:  0.22076550338949477\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9495782911624496\n",
            "\t f1 :  [0.72436814 0.97878213 0.73896458 0.83479681 0.81173469]\n",
            "\t average f1 :  0.8177292727414136\n",
            "\t confusion matrix :  [[ 1519   448    44    73   166]\n",
            " [  146 40779    65    56   118]\n",
            " [   48   222   678    14    45]\n",
            " [   65   542    14  2044    25]\n",
            " [  166   171    27    20  1591]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.012631386311517822\n",
            "Training metrics:\n",
            "\t accuracy :  0.9960004183871176\n",
            "\t f1 :  [0.96859721 0.99933013 0.97271623 0.99657956 0.97324108]\n",
            "\t average f1 :  0.9820928392807303\n",
            "\t confusion matrix :  [[  9608     82     63     13    145]\n",
            " [    65 167084     11      8     18]\n",
            " [    62     24   4403      7     41]\n",
            " [    20      7      4  10926      9]\n",
            " [   173      9     35      7   7947]]\n",
            "Validating..\n",
            "Validation loss:  0.24050374116216386\n",
            "Validation metrics:\n",
            "\t accuracy :  0.948722650042782\n",
            "\t f1 :  [0.72450533 0.97843918 0.70477137 0.82612279 0.82074522]\n",
            "\t average f1 :  0.8109167759359688\n",
            "\t confusion matrix :  [[ 1428   459    92    74   197]\n",
            " [   93 40797   120    47   107]\n",
            " [   26   218   709    16    38]\n",
            " [   42   591    27  2005    25]\n",
            " [  103   163    57    22  1630]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.014196935765169285\n",
            "Training metrics:\n",
            "\t accuracy :  0.9953047426589826\n",
            "\t f1 :  [0.96500025 0.99915221 0.96096096 0.99544378 0.97202669]\n",
            "\t average f1 :  0.9785167786352084\n",
            "\t confusion matrix :  [[  9526     98     80     21    149]\n",
            " [    72 166764     14      5     16]\n",
            " [    82     56   4320     11     56]\n",
            " [    24     10     11  10924     10]\n",
            " [   165     12     41      8   7940]]\n",
            "Validating..\n",
            "Validation loss:  0.21828733384609222\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9526545247117304\n",
            "\t f1 :  [0.73489519 0.97992132 0.73516949 0.85176282 0.82949547]\n",
            "\t average f1 :  0.8262488583711486\n",
            "\t confusion matrix :  [[ 1490   479    57    72   152]\n",
            " [   95 40849    75    62    83]\n",
            " [   38   222   694    19    34]\n",
            " [   48   480    18  2126    18]\n",
            " [  134   178    37    23  1603]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.009983839605141568\n",
            "Training metrics:\n",
            "\t accuracy :  0.9968565384001278\n",
            "\t f1 :  [0.97535497 0.99942783 0.97980801 0.9971661  0.97932975]\n",
            "\t average f1 :  0.9862173324688512\n",
            "\t confusion matrix :  [[  9617     70     51     14    116]\n",
            " [    55 166814     13      4     17]\n",
            " [    39     15   4440      4     35]\n",
            " [    17      7      3  10908      6]\n",
            " [   124     10     23      7   8007]]\n",
            "Validating..\n",
            "Validation loss:  0.24988561230046408\n",
            "Validation metrics:\n",
            "\t accuracy :  0.950882125249562\n",
            "\t f1 :  [0.71934325 0.97890164 0.7452881  0.836894   0.82497492]\n",
            "\t average f1 :  0.8210803838340087\n",
            "\t confusion matrix :  [[ 1402   513    56    68   211]\n",
            " [   84 40899    62    41    78]\n",
            " [   21   231   692    15    48]\n",
            " [   39   566    17  2037    31]\n",
            " [  102   188    23    17  1645]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.008393208961933851\n",
            "Training metrics:\n",
            "\t accuracy :  0.9973084917086592\n",
            "\t f1 :  [0.97887182 0.99953318 0.98319328 0.99799197 0.98103744]\n",
            "\t average f1 :  0.988125534777945\n",
            "\t confusion matrix :  [[  9683     54     39      9    116]\n",
            " [    42 167009     14      2     15]\n",
            " [    30     17   4446      2     24]\n",
            " [    13      3      5  10934      5]\n",
            " [   115      9     21      5   8019]]\n",
            "Validating..\n",
            "Validation loss:  0.2475725497518267\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9511265941408956\n",
            "\t f1 :  [0.72670208 0.97920211 0.73644252 0.84320557 0.82303883]\n",
            "\t average f1 :  0.8217182216873345\n",
            "\t confusion matrix :  [[ 1505   490    47    57   151]\n",
            " [  124 40867    65    41    67]\n",
            " [   49   230   679    11    38]\n",
            " [   59   531    16  2057    27]\n",
            " [  155   188    30    23  1579]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.007547141811637966\n",
            "Training metrics:\n",
            "\t accuracy :  0.9975212588214758\n",
            "\t f1 :  [0.98008816 0.99959897 0.98661356 0.99798627 0.98140623]\n",
            "\t average f1 :  0.9891386392832832\n",
            "\t confusion matrix :  [[  9672     43     32     11    113]\n",
            " [    40 167004      9      2     14]\n",
            " [    24     15   4459      5     19]\n",
            " [    12      2      0  10903      7]\n",
            " [   118      9     17      5   7970]]\n",
            "Validating..\n",
            "Validation loss:  0.2537709559713091\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9505969115430062\n",
            "\t f1 :  [0.72450233 0.97927113 0.72384937 0.84132694 0.82248366]\n",
            "\t average f1 :  0.8182866872843804\n",
            "\t confusion matrix :  [[ 1474   479    67    78   152]\n",
            " [  115 40817    91    73    68]\n",
            " [   39   216   692    24    36]\n",
            " [   42   503    19  2105    21]\n",
            " [  149   183    36    34  1573]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "#### Answer\n",
        "The final performance of bidirectional LSTMs is better than the FFNNs, a possible explanation for that is the LSTMs keep track of all context it sees from both ends not only a fixed size window like FFNNs.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "#### Answer\n",
        "The bidirectional performs better as it takes into account the prceeding and the following words, so it takes all the context of the words not only the prceeding ones like the unidrectional LSTM.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Comparsion\n",
        "\n",
        "|  | Training Accuracy | Validation Accuracy | Training f1 | Validation f1 | num of epochs |\n",
        "|----------|----------|----------|----------|----------|----------|\n",
        "| FFNNs (window_size=1)   |   0.9928   | 0.96104     | 0.9740     | 0.8609     | 5     |\n",
        "| FFNNs (window_size=2)  |   0.99456   | 0.9623     | 0.98149     | 0.8670     | 5     |\n",
        "| Bidirectional LSTM    | 0.9977     | 0.965     | 0.9923     | 0.8777     | 15     |\n",
        "| Unidirectional LSTM    | 0.9975     | 0.95059     | 0.9891     | 0.8182     | 15     |\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}